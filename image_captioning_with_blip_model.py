# -*- coding: utf-8 -*-
"""Image Captioning with BLIP Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sndX2Mbdbjv72U-meTCn837p4dOUsS1n
"""

# Install required libraries
!pip install transformers torch torchvision

# Import libraries
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import io
from google.colab import files

# Load the BLIP model and processor
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")

# Upload an image
uploaded = files.upload()

# Get the uploaded file name
file_name = next(iter(uploaded))

# Define the function to generate captions
def generate_caption_from_uploaded_image(file_name):
    # Load the uploaded image
    image = Image.open(file_name).convert("RGB")

    # Preprocess the image and generate caption
    inputs = processor(images=image, return_tensors="pt")
    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)

    return caption

# Generate caption for the uploaded image
caption = generate_caption_from_uploaded_image(file_name)
print("Generated Caption:", caption)